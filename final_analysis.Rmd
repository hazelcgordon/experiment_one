---
title: "Final Experiment 1 Analysis"
author: "Hazel C Gordon"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document lists all the models included in the final reported analysis of my main thesis (excluding the manipulation check), including supplementary materials.

Only the control of 'agreement' is needed in this document, however information about key demographics such as age, gender and education was collected and the variables are processed in the 'data_wrangle.Rmd' document. 

```{r libraries}

library(tidyverse)
library(stats)
library(sjPlot)
library(jtools)

```

```{r read in df}

#THIS ASSUMES YOU HAVE THE DATA DOWNLOADED AND SAVED INTO THE SAME R PROJECT

data <- read.csv("final_responses.csv")

```

## Exclusions

Excluded is any participant who failed both attention checks (already removed from dataset), any participant who completed the survey in less than half the median time, and one participant who was under 18 years old so did not complete the survey. This left 1,222 observations for analysis.

```{r}

data$Duration..in.seconds. <- as.numeric(data$Duration..in.seconds.)
data$AGE <- as.numeric(data$AGE)

data <- data %>%
  filter(Duration..in.seconds.>270)

data <- data %>%
  filter(AGE>17)

```

## Variable transformations

Predictors:

- orient_1_1 = trust_1
- orient_1_3 = trust_2
- orient_2_1 = trust_3
- orient_2_4 = trust_4
- orient_1_4 = distrust_1
- orient_1_5 = distrust_2
- orient_2_3 = distrust_3
- orient_2_5 = distrust_4
- DATA_ACCEPT (kept the same)

Outcomes:

- AD_TRUST_2 = manipulative
- AD_TRUST_1 = credible


```{r agree disagree transformations}

#every agree-disagree item on a scale of 1-7

df1 <- data %>%
  data.frame() %>%
  select(ORIENT_1_1, ORIENT_1_3, ORIENT_2_1, ORIENT_2_4, ORIENT_1_4, ORIENT_1_5, ORIENT_2_3, ORIENT_2_5, COMP_1, COMP_2, COMP_3, COMP_4, BENEV_1, BENEV_2, BENEV_3, BENEV_4, INTEG_1, INTEG_2, INTEG_3, INTEG_4, COMP_1, COMP_2, COMP_3, COMP_4, AD_TRUST_1, AD_TRUST_2, AD_TRUST_4, AD_TRUST_5
         )

df1 <- df1 %>%
  select(ORIENT_1_1, ORIENT_1_3, ORIENT_2_1, ORIENT_2_4, ORIENT_1_4, ORIENT_1_5, ORIENT_2_3, ORIENT_2_5, COMP_1, COMP_2, COMP_3, COMP_4, BENEV_1, BENEV_2, BENEV_3, BENEV_4, INTEG_1, INTEG_2, INTEG_3, INTEG_4, COMP_1, COMP_2, COMP_3, COMP_4, AD_TRUST_1, AD_TRUST_2, AD_TRUST_4, AD_TRUST_5) %>%
  mutate_all(~c(1, 2, 3, 4, 5, 6, 7)[match(.x, c("Strongly disagree", 
                                                 "Disagree", "Somewhat disagree",
                                                 "Neither agree nor disagree",
                                                 "Somewhat agree", "Agree",
                                                 "Strongly agree"))])

#data acceptability on a scale of 1 to 7

df2 <- data %>%
  data.frame() %>%
  select(DATA_ACCEPT)

df2 <- df2 %>%
  select(DATA_ACCEPT) %>%
  mutate_all(~c(1, 2, 3, 4, 5, 6, 7)[match(.x, c("Not at all acceptable", "Mostly unacceptable ", "Somewhat unacceptable", "Neither acceptable nor unacceptable", "Somewhat acceptable", "Mostly acceptable", "Completely acceptable "))])

#self-targeting manipulation check

df4 <- data %>%
  data.frame() %>%
  select(SELF_TAR)

df4 <- df4 %>%
  select(SELF_TAR) %>%
  mutate_all(~c(1, 2, 3, 4, 5)[match(.x, c("Not at all targeted at me", "Mostly not targeted ", "Somewhat targeted", "Mostly targeted", "Completely targeted at me" 
))])

#rename to something more informative

df1 <- df1 %>%
  rename("TRUST_1" = "ORIENT_1_1",
         "TRUST_2" = "ORIENT_1_3",
         "TRUST_3" = "ORIENT_2_1",
         "TRUST_4" = "ORIENT_2_4",
         "DISTRUST_1" = "ORIENT_1_4",
         "DISTRUST_2" = "ORIENT_1_5",
         "DISTRUST_3" = "ORIENT_2_3",
         "DISTRUST_4" = "ORIENT_2_5")

df1 <- df1 %>%
  rename("COMPTRUST_1" = "COMP_1",
         "COMPTRUST_2" = "COMP_4",
         "COMPDIS_1" = "COMP_2",
         "COMPDIS_2" = "COMP_3",
         "INTTRUST_1" = "INTEG_1",
         "INTTRUST_2" = "INTEG_4",
         "INTDIS_1" = "INTEG_2",
         "INTDIS_2" = "INTEG_3",
         "BENTRUST_1" = "BENEV_2",
         "BENTRUST_2" = "BENEV_4",
         "BENDIS_1" = "BENEV_1",
         "BENDIS_2" = "BENEV_3",
         "MANIPULATIVE" = "AD_TRUST_2",
         "CREDIBLE" = "AD_TRUST_1")

```

```{r agreement}

#agreement on scale of 1 to 5 

df3 <- data %>%
  data.frame() %>%
  select(AGREE)

df3 <- df3 %>%
  select(AGREE) %>%
  mutate_all(~c(1, 2, 3, 4, 5)[match(.x, c("Completely disagree", "Somewhat disagree", "Neither agree or disagree", "Somewhat agree", "Completely agree"
))])

```

```{r recall}

#the code below converts the memory variables into recall categories by experimental condition, creating a 4-factor level for each

memory <- data %>%
  data.frame() %>%
  select(Condition, MEM_PAID, MEM_TAR)

memory$Condition <- as.numeric(memory$Condition)

memory$MEM_PAID[memory$MEM_PAID == 'Yes'] <- '1'
memory$MEM_PAID[memory$MEM_PAID == 'No'] <- '0'
memory$MEM_TAR[memory$MEM_TAR == 'Yes'] <- '1'
memory$MEM_TAR[memory$MEM_TAR == 'No'] <- '0'

memory$MEM_PAID <- as.numeric(memory$MEM_PAID)
memory$MEM_TAR <- as.numeric(memory$MEM_TAR)

memory$PAID_diff<-(memory$Condition-memory$MEM_PAID)

memory$PAID_diff[memory$PAID_diff == '0'] <- 'correct'
memory$PAID_diff[memory$PAID_diff == '1'] <- 'incorrect_exp'
memory$PAID_diff[memory$PAID_diff == '-1'] <- 'incorrect_control'

memory$TAR_diff<-(memory$Condition-memory$MEM_TAR)

memory$TAR_diff[memory$TAR_diff == '0'] <- 'correct'
memory$TAR_diff[memory$TAR_diff == '1'] <- 'incorrect_exp'
memory$TAR_diff[memory$TAR_diff == '-1'] <- 'incorrect_control'

#setting condition as a categorical dummy variable

memory$Condition[memory$Condition == '0'] <- 'control'
memory$Condition[memory$Condition == '1'] <- 'transparent'

memory$Condition <- as.factor(memory$Condition)

memory <- memory %>%
  mutate(
    Condition = relevel(factor(Condition, levels = 
                                 c("control", 
                                   "transparent"
                                 )), ref = "control"
    )
  )

#splitting into a 4-level factor

rename_recall <- function(df, response_col, condition_col) {
  response_col_sym <- rlang::sym(response_col)
  condition_col_sym <- rlang::sym(condition_col)
  
  df <- df %>%
    mutate(!!response_col_sym := case_when(
      !!response_col_sym == "correct" & !!condition_col_sym == "control" ~ "correct_control",
      !!response_col_sym == "correct" & !!condition_col_sym == "transparent" ~ "correct_exp",
      TRUE ~ as.character(!!response_col_sym)
    ))
  
  return(df)
}

memory <- rename_recall(memory, "PAID_diff", "Condition")
memory <- rename_recall(memory, "TAR_diff", "Condition")

# resetting the factor levels, making incorrect recall exp the reference

memory <- memory %>%
  mutate(PAID_diff = factor(PAID_diff, 
                            levels = c("correct_exp", "incorrect_exp", "incorrect_control", 
                                       "correct_control")))
memory <- memory %>%
  mutate(TAR_diff = factor(TAR_diff, 
                            levels = c("correct_exp", "incorrect_exp", "incorrect_control", 
                                       "correct_control")))

```

```{r}

#combining the dataframes

final <- cbind(df1, df2, df3, df4, memory)

```

## Combined-score measures

The code below averages all combined scores to form the final versions of the variables, ready for analysis.

```{r}

#first, create a distrust and trust orientation variable on seperate dimensions for the supplementary analysis

final <- final %>%
  rowwise() %>%
  mutate(distrust_orient = mean(c(DISTRUST_1, DISTRUST_2, DISTRUST_3, DISTRUST_4)))

final <- final %>%
  rowwise() %>%
  mutate(trust_orient = mean(c(TRUST_1, TRUST_2, TRUST_3, TRUST_4)))

#function for reverse coding

reverse_code <- function(response) {
  # Define the mapping from original to reversed scores
  mapping <- c(1, 2, 3, 4, 5, 6, 7)
  names(mapping) <- c(7, 6, 5, 4, 3, 2, 1)
  
  # Use the response as a name to look up in the mapping
  return(as.numeric(names(mapping)[match(response, mapping)]))
}

# Apply the function to the specified columns
final <- final %>%
  mutate_at(vars(DISTRUST_1, DISTRUST_2, DISTRUST_3, DISTRUST_4), reverse_code)

#change names to account for reversal

final <- final %>%
  mutate(DISTRUST_1rev = DISTRUST_1,
         DISTRUST_2rev = DISTRUST_2,
         DISTRUST_3rev = DISTRUST_3,
         DISTRUST_4rev = DISTRUST_4)

#Mean scoring political trust orientation on one dimension

final <- final %>%
  rowwise() %>%
  mutate(pol_orientation = mean(c(TRUST_1, TRUST_2, TRUST_3, TRUST_4, DISTRUST_1rev, DISTRUST_2rev, DISTRUST_3rev, DISTRUST_4rev)))

#Mean scoring competence: trust and distrust

final <- final %>%
  rowwise() %>%
  mutate(comp_distrust = mean(c(COMPDIS_1, COMPDIS_2)))

final <- final %>%
  rowwise() %>%
  mutate(comp_trust = mean(c(COMPTRUST_1, COMPTRUST_2)))

#Mean scoring integrity

final <- final %>%
  rowwise() %>%
  mutate(int_distrust = mean(c(INTDIS_1, INTDIS_2)))

final <- final %>%
  rowwise() %>%
  mutate(int_trust = mean(c(INTTRUST_1, INTTRUST_2)))

#Mean scoring benevolence 

final <- final %>%
  rowwise() %>%
  mutate(ben_distrust = mean(c(BENDIS_1, BENDIS_2)))

final <- final %>%
  rowwise() %>%
  mutate(ben_trust = mean(c(BENTRUST_1, BENTRUST_2)))

#transparency items mean scored

final <- final %>%
  rowwise() %>%
  mutate(TRANSPARENT = mean(c(AD_TRUST_4, AD_TRUST_5)))

```

## Descriptive statistics

```{r include=FALSE}

#control

dfcontrol <- final %>%
  data.frame() %>%
  select(Condition, pol_orientation, DATA_ACCEPT, AGREE, MANIPULATIVE, CREDIBLE, comp_trust, int_trust, ben_trust, comp_distrust, int_distrust, ben_distrust)

dfcontrol <- dfcontrol %>%
  subset(Condition == "control")

dftran <- final %>%
  data.frame() %>%
  select(Condition, pol_orientation, DATA_ACCEPT, AGREE, MANIPULATIVE, CREDIBLE, comp_trust, int_trust, ben_trust, comp_distrust, int_distrust, ben_distrust)

dftran <- dftran %>%
  subset(Condition == "transparent")

dfcontrol <- subset(dfcontrol, select = -Condition)
dftran <- subset(dftran, select = -Condition)

```

### Table with mean, SD, min, max

```{r descriptive stat tables, echo=FALSE}

#create new columns of descriptive statistics

summ_control <- c("pol_orientation", "DATA_ACCEPT", "AGREE", "MANIPULATIVE", "CREDIBLE", "comp_trust", "int_trust", "ben_trust", "comp_distrust", "int_distrust", "ben_distrust")

d_control <- data.frame(
  Variable = summ_control,
  Mean = sapply(dfcontrol[, summ_control], mean, na.rm = TRUE),
  SD = sapply(dfcontrol[, summ_control], sd, na.rm = TRUE),      
  Min = sapply(dfcontrol[, summ_control], min, na.rm = TRUE),
  Max = sapply(dfcontrol[, summ_control], max, na.rm = TRUE)
)

summ_tran <- c("pol_orientation", "DATA_ACCEPT", "AGREE", "MANIPULATIVE", "CREDIBLE", "comp_trust", "int_trust", "ben_trust", "comp_distrust", "int_distrust", "ben_distrust")

d_tran <- data.frame(
  Variable = summ_tran,
  Mean = sapply(dftran[, summ_tran], mean, na.rm = TRUE),
  SD = sapply(dftran[, summ_tran], sd, na.rm = TRUE),    
  Min = sapply(dftran[, summ_tran], min, na.rm = TRUE),
  Max = sapply(dftran[, summ_tran], max, na.rm = TRUE)
)

new_row_names <- c("Political trust orientation", "Perceived acceptability of data use", "Agreement with campaign", "Perceived manipulation", "Perceived credibility", "Perceived competence (trust)", "Perceived integrity (trust)", "Perceived benevolence (trust)", "Perceived competence (distrust)", "Perceived integrity (distrust)", "Perceived benevolence (distrust)")

d_control$Variable <- new_row_names
d_tran$Variable <- new_row_names


#present the results side by side in a table

library(gt)

controltable <- gt(d_control) |>
  tab_header(
    title = "Control group") |>
  cols_label(
    Variable = "") |>
  fmt_number(decimals = 2) |>
  tab_source_note(source_note = md("*From a total of 611 respondents*")) |>
  cols_align(align = "center", columns = c(Mean, SD, Min, Max)) |>
  cols_align(align = "left", columns = Variable) |>
  cols_width(c(Variable) ~ px(150)) |>
  tab_options(
    table.border.top.color = "black",
    table.border.bottom.color = "black",
    heading.border.bottom.color = "black",
    column_labels.border.bottom.color = "black",
    table_body.border.bottom.color = "black",
    heading.background.color = "seagreen",
    source_notes.background.color = "seagreen"
  )

controltable

trantable <- gt(d_tran) |>
  tab_header(
    title = "Experimental group") |>
  cols_label(
    Variable = "") |>
  fmt_number(decimals = 2) |>
  tab_source_note(source_note = md("*From a total of 611 respondents*")) |>
  cols_align(align = "center", columns = c(Mean, SD, Min, Max)) |>
  cols_align(align = "left", columns = Variable) |>
  cols_width(c(Variable) ~ px(150)) |>
  tab_options(
    table.border.top.color = "black",
    table.border.bottom.color = "black",
    heading.border.bottom.color = "black",
    column_labels.border.bottom.color = "black",
    table_body.border.bottom.color = "black",
    heading.background.color = "seagreen",
    source_notes.background.color = "seagreen"
  )

trantable

```

## Recall

```{r sponsorship recall, echo=FALSE}

# Calculate percentages
recall_percentages <- final %>%
  group_by(Condition, PAID_diff) %>%
  summarize(count = n(), .groups = 'drop') %>%
  group_by(Condition) %>%
  mutate(percentage = count / sum(count) * 100)

#convert to character

recall_percentages$PAID_diff <- as.character(recall_percentages$PAID_diff)

#rename the variables

recall_percentages$PAID_diff[recall_percentages$PAID_diff == 'correct_exp'] <- 'Recall'
recall_percentages$PAID_diff[recall_percentages$PAID_diff == 'incorrect_exp'] <- 'No Recall'
recall_percentages$PAID_diff[recall_percentages$PAID_diff == 'incorrect_control'] <- 'Recall'
recall_percentages$PAID_diff[recall_percentages$PAID_diff == 'correct_control'] <- 'No Recall'

#capitalise first name of word for better aesthetics

recall_percentages$Condition <- as.character(recall_percentages$Condition)

recall_percentages$Condition[recall_percentages$Condition == 'control'] <- 'Control'
recall_percentages$Condition[recall_percentages$Condition == 'transparent'] <- 'Transparent'

#convert back to factor

recall_percentages$PAID_diff <- as.factor(recall_percentages$PAID_diff)
recall_percentages$Condition <- as.factor(recall_percentages$Condition)

#create the plot

# Define custom colors
custom_colors <- c("No Recall" = "sienna2", "Recall" = "lightblue")

# Plot the data
sponsorship_figure <- ggplot(recall_percentages, aes(x = Condition, y = count, fill = PAID_diff)) +
  geom_bar(stat = "identity", position = "stack", width = 0.3) +  
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5),
            size = 6) +
  labs(y = "Count", x = "Condition", fill = "") +
  theme_minimal() +
  scale_fill_manual(values = custom_colors) + 
  scale_x_discrete(name = "Disclosure Condition") +  
  theme(
    axis.title.x = element_text(size = rel(0.9)),
    legend.position = "top",
    legend.direction = "horizontal",
    axis.title = element_text(size = 20),   # Axis title size
        axis.text = element_text(size = 20), 
        axis.text.y = element_text(size = 20),
        legend.title = element_text(size = 18), # Legend title size
        legend.text = element_text(size = 18))

sponsorship_figure

```

```{r targeting recall, echo=FALSE}

# Calculate percentages
target_percentages <- final %>%
  group_by(Condition, TAR_diff) %>%
  summarize(count = n(), .groups = 'drop') %>%
  group_by(Condition) %>%
  mutate(percentage = count / sum(count) * 100)

#convert to character

target_percentages$TAR_diff <- as.character(target_percentages$TAR_diff)

#rename the variables

target_percentages$TAR_diff[target_percentages$TAR_diff == 'correct_exp'] <- 'Recall'
target_percentages$TAR_diff[target_percentages$TAR_diff == 'incorrect_exp'] <- 'No Recall'
target_percentages$TAR_diff[target_percentages$TAR_diff == 'incorrect_control'] <- 'Recall'
target_percentages$TAR_diff[target_percentages$TAR_diff == 'correct_control'] <- 'No Recall'

#capitalise first name of word for better aesthetics

target_percentages$Condition <- as.character(target_percentages$Condition)

target_percentages$Condition[target_percentages$Condition == 'control'] <- 'Control'
target_percentages$Condition[target_percentages$Condition == 'transparent'] <- 'Transparent'

#convert back to factor

target_percentages$TAR_diff <- as.factor(target_percentages$TAR_diff)
target_percentages$Condition <- as.factor(target_percentages$Condition)

#create the plot

# Plot the data
target_figure <- ggplot(target_percentages, aes(x = Condition, y = count, fill = TAR_diff)) +
  geom_bar(stat = "identity", position = "stack", width = 0.3) +  
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5),
            size = 6) +
  labs(y = "Count", x = "Condition", fill = "") +
  theme_minimal() +
  scale_fill_manual(values = custom_colors) + 
  scale_x_discrete(name = "Disclosure Condition") +  
  theme(
    axis.title.x = element_text(size = rel(0.9)),
    legend.position = "top",
    legend.direction = "horizontal",
    axis.title = element_text(size = 20),   # Axis title size
        axis.text = element_text(size = 20), 
        axis.text.y = element_text(size = 20),
        legend.title = element_text(size = 18), # Legend title size
        legend.text = element_text(size = 18))

target_figure

```


```{r save plots, eval=FALSE}

ggsave("figures/sponsorship_recall_descriptive.png", plot = sponsorship_figure, width = 9, height = 6, units = "in", dpi = 300)

ggsave("figures/targeting_recall_descriptive.png", plot = target_figure, width = 9, height = 6, units = "in", dpi = 300)


```

## Manipulation checks

```{r agreement and ideology, echo=FALSE}

#test is agreement is predicted by ideology

library(MASS)

agree_ideology <- rlm(data=final, AGREE ~ LEFT_RIGHT)

tab_model(agree_ideology)
summary(agree_ideology)

```

```{r plotting, eval=FALSE, include=FALSE}

library(multcomp)

pairwise_comp_1 <- glht(agree_ideology, linfct = mcp(LEFT_RIGHT = "Tukey"))

summary(pairwise_comp_1, test = adjusted("bonferroni"))

confint(pairwise_comp_1, test = adjusted("bonferroni"))

# Extract the summary of the glht object
summary_comp <- summary(pairwise_comp_1)

# Extract the coefficients (estimated differences)
coefficients <- summary_comp$test$coef
confint <- confint(summary_comp)  # Get confidence intervals using confint()
confint_lower <- confint$confint[, "lwr"]  # Lower bounds
confint_upper <- confint$confint[, "upr"]  # Upper bounds
p_values <- summary_comp$test$pvalues

# Create a data frame for plotting
comparison_df <- data.frame(
  Comparison = names(coefficients),  # Pairwise comparisons
  Estimate = coefficients,           # Estimated differences
  CI_Lower = confint_lower,           # Lower bound of CI
  CI_Upper = confint_upper,           # Upper bound of CI
  p_value = p_values                  # Adjusted p-values
)

# Create a ggplot of the coefficients and their confidence intervals
ggplot(comparison_df, aes(x = Estimate, y = Comparison)) +
  geom_point() +  # Plot the coefficient estimates as points
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.2) +  # Horizontal error bars for CIs
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Vertical line at 0 for reference
  labs(title = "Pairwise Comparisons (95% CI)", 
       x = "Log-Odds Estimate", y = "Comparison") +
  theme_minimal()  # Clean theme for the plot

```

## Hypothesis Testing

H1a: The presence of a disclosure will impact perceptions of integrity and benevolence, but not competence

H1b: The presence of a disclosure will impact perceptions of both credibility (a trust perception) and manipulativeness (a distrust perception)

H1c: The presence of a disclosure will impact evaluations of distrust more than trust

H2a: political trust orientation will moderate the association between viewing a disclosure on distrust evaluations, with the most politically distrusting citizens exhibiting the largest backfire effect in response to viewing a disclosure

H2b: an individual's belief about the acceptability of personalised data use in political advertising will moderate the association between viewing a disclosure on distrust evaluations, with those who perceive it as the most unacceptable exhibiting the largest backfire effect in response to viewing a disclosure 

```{r competence trust dimension, echo=FALSE}

#beginning with the outcome 'perceived competence, trust dimension' the following models are tested

tc_simple <- lm(data = final, comp_trust ~ Condition + AGREE)
tc_int1 <- lm(data = final, comp_trust ~ Condition*pol_orientation + AGREE)
tc_int2 <- lm(data = final, comp_trust ~ Condition*pol_orientation + Condition*DATA_ACCEPT + AGREE)

#the test below shows the interaction effect significantly improves the fit of the model

anova(tc_simple, tc_int1)
anova(tc_int1, tc_int2)

#most complex model achieves the lowest AIC, supporting that the higher complexity does not significantly disadvantage model fit

AIC(tc_simple, tc_int1, tc_int2)

#summary of models

summary(tc_simple)
summary(tc_int1)

tc_table <- tab_model(tc_int2, show.se = TRUE)
tc_table

#adjusting the p-values for the false discovery rate

# Extract p-values from the summary of the model
p_vals <- summary(tc_int2)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

```{r integrity trust dimension, echo=FALSE}

#on the outcome 'perceived integrity, trust dimension' the following models are tested

tc_simple <- lm(data = final, int_trust ~ Condition + AGREE)
tc_int1 <- lm(data = final, int_trust ~ Condition*pol_orientation + AGREE)
tc_int2 <- lm(data = final, int_trust ~ Condition*pol_orientation + Condition*DATA_ACCEPT + AGREE)

#the test below shows the interaction effect significantly improves the fit of the model

anova(tc_simple, tc_int1)
anova(tc_int1, tc_int2)

#most complex model achieves the lowest AIC, supporting that the higher complexity does not significantly disadvantage model fit

AIC(tc_simple, tc_int1, tc_int2)

#summary of models

summary(tc_simple)
summary(tc_int1)

tc_table <- tab_model(tc_int2, show.se = TRUE)
tc_table
summary(tc_int2)

#adjusting the p-values for the false discovery rate

# Extract p-values from the summary of the model
p_vals <- summary(tc_int2)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

```{r benevolence trust dimension, echo=FALSE}

#on the outcome 'perceived benevolence, trust dimension' the following models are tested

tc_simple <- lm(data = final, ben_trust ~ Condition + AGREE)
tc_int1 <- lm(data = final, ben_trust ~ Condition*pol_orientation + AGREE)
tc_int2 <- lm(data = final, ben_trust ~ Condition*pol_orientation + Condition*DATA_ACCEPT + AGREE)

#the test below shows the interaction effect significantly improves the fit of the model

anova(tc_simple, tc_int1)
anova(tc_int1, tc_int2)

#most complex model achieves the lowest AIC, supporting that the higher complexity does not significantly disadvantage model fit

AIC(tc_simple, tc_int1, tc_int2)

#summary of models

summary(tc_simple)
summary(tc_int1)

tc_table <- tab_model(tc_int2, show.se = TRUE)
tc_table
summary(tc_int2)

#adjusting the p-values for the false discovery rate

# Extract p-values from the summary of the model
p_vals <- summary(tc_int2)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

```{r credibility, echo=FALSE}

#on the outcome 'perceived credibility' the following models are tested

tc_simple <- lm(data = final, CREDIBLE ~ Condition + AGREE)
tc_int1 <- lm(data = final, CREDIBLE ~ Condition*pol_orientation + AGREE)
tc_int2 <- lm(data = final, CREDIBLE ~ Condition*pol_orientation + Condition*DATA_ACCEPT + AGREE)

#the test below shows the interaction effect significantly improves the fit of the model

anova(tc_simple, tc_int1)
anova(tc_int1, tc_int2)

#most complex model achieves the lowest AIC, supporting that the higher complexity does not significantly disadvantage model fit

AIC(tc_simple, tc_int1, tc_int2)

#summary of models

summary(tc_simple)
summary(tc_int1)

tc_table <- tab_model(tc_int2, show.se = TRUE)
tc_table
summary(tc_int2)

#adjusting the p-values for the false discovery rate

# Extract p-values from the summary of the model
p_vals <- summary(tc_int2)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

```{r competence distrust dimension, echo=FALSE}

#on the outcome 'perceived competence, distrust dimension' the following models are tested

tc_simple <- lm(data = final, comp_distrust ~ Condition + AGREE)
tc_int1 <- lm(data = final, comp_distrust ~ Condition*pol_orientation + AGREE)
tc_int2 <- lm(data = final, comp_distrust ~ Condition*pol_orientation + Condition*DATA_ACCEPT + AGREE)

#the test below shows the interaction effect significantly improves the fit of the model

anova(tc_simple, tc_int1)
anova(tc_int1, tc_int2)

AIC(tc_simple, tc_int1, tc_int2)

#summary of models

summary(tc_simple)
summary(tc_int1)

tc_table <- tab_model(tc_int2, show.se = TRUE)
tc_table
summary(tc_int2)

#adjusting the p-values for the false discovery rate

# Extract p-values from the summary of the model
p_vals <- summary(tc_int2)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

```{r integrity distrust dimension, echo=FALSE}

#on the outcome 'perceived integrity, distrust dimension' the following models are tested

tc_simple <- lm(data = final, int_distrust ~ Condition + AGREE)
tc_int1 <- lm(data = final, int_distrust ~ Condition*pol_orientation + AGREE)
tc_int2 <- lm(data = final, int_distrust ~ Condition*pol_orientation + Condition*DATA_ACCEPT + AGREE)

#the test below shows the interaction effect significantly improves the fit of the model

anova(tc_simple, tc_int1)
anova(tc_int1, tc_int2)

AIC(tc_simple, tc_int1, tc_int2)

#summary of models

summary(tc_simple)
summary(tc_int1)

tc_table <- tab_model(tc_int2, show.se = TRUE)
tc_table
summary(tc_int2)

#adjusting the p-values for the false discovery rate

# Extract p-values from the summary of the model
p_vals <- summary(tc_int2)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

```{r benevolence distrust dimension, echo=FALSE}

#on the outcome 'perceived benevolence, distrust dimension' the following models are tested

tc_simple <- lm(data = final, ben_distrust ~ Condition + AGREE)
tc_int1 <- lm(data = final, ben_distrust ~ Condition*pol_orientation + AGREE)
tc_int2 <- lm(data = final, ben_distrust ~ Condition*pol_orientation + Condition*DATA_ACCEPT + AGREE)

#the test below shows the interaction effect significantly improves the fit of the model

anova(tc_simple, tc_int1)
anova(tc_int1, tc_int2)

AIC(tc_simple, tc_int1, tc_int2)

#summary of models

summary(tc_simple)
summary(tc_int1)

tc_table <- tab_model(tc_int2, show.se = TRUE)
tc_table
summary(tc_int2)

#adjusting the p-values for the false discovery rate

# Extract p-values from the summary of the model
p_vals <- summary(tc_int2)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

```{r manipulative, echo=FALSE}

#on the outcome 'perceived manipulation' the following models are tested

tc_simple <- lm(data = final, MANIPULATIVE ~ Condition + AGREE)
tc_int1 <- lm(data = final, MANIPULATIVE ~ Condition*pol_orientation + AGREE)
tc_int2 <- lm(data = final, MANIPULATIVE ~ Condition*pol_orientation + Condition*DATA_ACCEPT + AGREE)

#the test below shows the interaction effect significantly improves the fit of the model

anova(tc_simple, tc_int1)
anova(tc_int1, tc_int2)

AIC(tc_simple, tc_int1, tc_int2)

#summary of models

summary(tc_simple)
summary(tc_int1)

tc_table <- tab_model(tc_int2, show.se = TRUE)
tc_table
summary(tc_int2)

#adjusting the p-values for the false discovery rate

# Extract p-values from the summary of the model
p_vals <- summary(tc_int2)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

Effect of recall

The models below test if recalling the disclosure impacted the trust perceptions, to explore how difference in processing of the information might have impacted perceptions.

Sponsorship recall models

```{r sponsorship recall}

recall_comp_t <- lm(comp_trust ~ PAID_diff + AGREE, data=final)
recall_int_t <- lm(int_trust ~ PAID_diff + AGREE, data=final)
recall_ben_t <- lm(ben_trust ~ PAID_diff + AGREE, data=final)
recall_cred_t <- lm(CREDIBLE ~ PAID_diff + AGREE, data = final)

recall_comp_d <- lm(comp_distrust ~ PAID_diff + AGREE, data=final)
recall_int_d <- lm(int_distrust ~ PAID_diff + AGREE, data=final)
recall_ben_d <- lm(ben_distrust ~ PAID_diff + AGREE, data=final)
recall_man_d <- lm(MANIPULATIVE ~ PAID_diff + AGREE, data = final)

tab_model(recall_comp_d, show.se = TRUE)
tab_model(recall_man_d, show.se = TRUE)

#adjusted p-value
# Extract p-values from the summary of the model
p_vals <- summary(recall_ben_d)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

#adjusted p-value
# Extract p-values from the summary of the model
p_vals <- summary(recall_man_d)$coefficients[, "Pr(>|t|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

Targeting recall models

```{r targeting recall}

target_comp_t <- lm(comp_trust ~ TAR_diff + AGREE, data=final)
target_int_t <- lm(int_trust ~ TAR_diff + AGREE, data=final)
target_ben_t <- lm(ben_trust ~ TAR_diff + AGREE, data=final)
target_cred_t <- lm(CREDIBLE ~ TAR_diff + AGREE, data = final)

target_comp_d <- lm(comp_distrust ~ TAR_diff + AGREE, data=final)
target_int_d <- lm(int_distrust ~ TAR_diff + AGREE, data=final)
target_ben_d <- lm(ben_distrust ~ TAR_diff + AGREE, data=final)
target_man_d <- lm(MANIPULATIVE ~ TAR_diff + AGREE, data = final)

tab_model(target_ben_d, show.se = TRUE)
tab_model(target_man_d, show.se = TRUE)

```

Sponsorship plot, trust dimensions

```{r plotting sponsorship trust, echo=FALSE}

#make trust plot

dark_colors <- c("Competence (trust)" = "#006400", "Integrity (trust)" = "sienna3", "Benevolence (trust)" = "#00008B", "Credible" = "purple")
  
magnitude <- plot_summs(recall_comp_t, recall_int_t, recall_ben_t, recall_cred_t,
                        model.names = c("Competence (trust)", "Integrity (trust)", "Benevolence (trust)", "Credible"), 
                        coefs = c("Shown sponsorship disclosure:\n incorrectly did not recall \n (ref = shown disclosure, correct recall)" = "PAID_diffincorrect_exp",
                                  "Not shown sponsorship disclosure:\n correctly did not recall \n (ref = shown disclosure, correct recall)" = "PAID_diffincorrect_control",
                                  "Not shown sponsorship disclosure:\n incorrectly recalled \n (ref = shown disclosure, correct recall)" = "PAID_diffcorrect_control",
                                  "Agreement with campaign \n message" = "AGREE"),
                        exp = TRUE, omit.coefs = 
                          c("(Intercept)"),
                        colors = dark_colors) +
  labs(x = "Exponentiated Coefficient") +
  theme(axis.title = element_text(size = 16),   # Axis title size
        axis.text = element_text(size = 16), 
        axis.text.y = element_text(size = 16),
        legend.title = element_text(size = 18), # Legend title size
        legend.text = element_text(size = 18))

magnitude

```

Targeting recall plot, trust dimensions

```{r plotting targeting trust, echo=FALSE}

#make trust plot

dark_colors <- c("Competence (trust)" = "#006400", "Integrity (trust)" = "sienna3", "Benevolence (trust)" = "#00008B", "Credible" = "purple")
  
target_magnitude <- plot_summs(target_comp_t, target_int_t, target_ben_t, target_cred_t,
                        model.names = c("Competence (trust)", "Integrity (trust)", "Benevolence (trust)", "Credible"), 
                        coefs = c("Shown targeting disclosure:\n incorrectly did not recall \n (ref = shown disclosure, correct recall)" = "TAR_diffincorrect_exp",
                                  "Not shown targeting disclosure:\n correctly did not recall \n (ref = shown disclosure, correct recall)" = "TAR_diffincorrect_control",
                                  "Not shown targeting disclosure:\n incorrectly recalled \n (ref = shown disclosure, correct recall)" = "TAR_diffcorrect_control",
                                  "Agreement with campaign \n message" = "AGREE"),
                        exp = TRUE, omit.coefs = 
                          c("(Intercept)"),
                        colors = dark_colors) +
  labs(x = "Exponentiated Coefficient") +
  theme(axis.title = element_text(size = 16),   # Axis title size
        axis.text = element_text(size = 16), 
        axis.text.y = element_text(size = 16),
        legend.title = element_text(size = 18), # Legend title size
        legend.text = element_text(size = 18))

target_magnitude

```

Sponsorship, distrust dimensions

```{r distrust plot sponsorship, echo=FALSE}

#make distrust plot

dark_colors1 <- c("Competence (distrust)" = "#006400", "Integrity (distrust)" = "sienna3", "Benevolence (distrust)" = "#00008B", "Manipulative" = "purple")
  
magnitude1 <- plot_summs(recall_comp_d, recall_int_d, recall_ben_d, recall_man_d,
                        model.names = c("Competence (distrust)", "Integrity (distrust)", "Benevolence (distrust)", "Manipulative"), 
                        coefs = c("Shown sponsorship disclosure:\n incorrectly did not recall \n (ref = shown disclosure, correct recall)" = "PAID_diffincorrect_exp",
                                  "Not shown sponsorship disclosure:\n correctly did not recall \n (ref = shown disclosure, correct recall)" = "PAID_diffincorrect_control",
                                  "Not shown sponsorship disclosure:\n incorrectly recalled \n (ref = shown disclosure, correct recall)" = "PAID_diffcorrect_control",
                                  "Agreement with campaign \n message" = "AGREE"),
                        exp = TRUE, omit.coefs = 
                          c("(Intercept)"),
                        colors = dark_colors1) +
  labs(x = "Exponentiated Coefficient") +
  theme(axis.title = element_text(size = 16),   # Axis title size
        axis.text = element_text(size = 16), 
        axis.text.y = element_text(size = 16),
        legend.title = element_text(size = 18), # Legend title size
        legend.text = element_text(size = 18))

magnitude1

```

Targeting recall plot, trust dimensions

```{r plotting targeting trust, echo=FALSE}

#make trust plot
  
target_magnitude1 <- plot_summs(target_comp_d, target_int_d, target_ben_d, target_man_d,
                        model.names = c("Competence (distrust)", "Integrity (distrust)", "Benevolence (distrust)", "Manipulative"), 
                        coefs = c("Shown targeting disclosure:\n incorrectly did not recall \n (ref = shown disclosure, correct recall)" = "TAR_diffincorrect_exp",
                                  "Not shown targeting disclosure:\n correctly did not recall \n (ref = shown disclosure, correct recall)" = "TAR_diffincorrect_control",
                                  "Not shown targeting disclosure:\n incorrectly recalled \n (ref = shown disclosure, correct recall)" = "TAR_diffcorrect_control",
                                  "Agreement with campaign \n message" = "AGREE"),
                        exp = TRUE, omit.coefs = 
                          c("(Intercept)"),
                        colors = dark_colors1) +
  labs(x = "Exponentiated Coefficient") +
  theme(axis.title = element_text(size = 16),   # Axis title size
        axis.text = element_text(size = 16), 
        axis.text.y = element_text(size = 16),
        legend.title = element_text(size = 18), # Legend title size
        legend.text = element_text(size = 18))

target_magnitude1

```


```{r saving the plots, eval=FALSE}

ggsave("figures/sponsorship_trust_perceptions.png", plot = magnitude, width = 12, height = 8, units = "in", dpi = 300)

ggsave("figures/sponsorship_distrust_perceptions.png", plot = magnitude1, width = 12, height = 8, units = "in", dpi = 300)

ggsave("figures/targeting_trust_perceptions.png", plot = target_magnitude, width = 12, height = 8, units = "in", dpi = 300)

ggsave("figures/targeting_distrust_perceptions.png", plot = target_magnitude1, width = 12, height = 8, units = "in", dpi = 300)

```

## What is the difference in demographics between those who reported and did not report viewing the disclaimer?

Aspects compared: agreement with campaign material, time spent on page, political interest, political ideology, trust in regulation, gender, age, education.

```{r political attitude}

#political interest

df1 <- data %>%
  data.frame() %>%
  select(POL_INT)

#make into a factor

df1 <- df1 %>%
  mutate(
    POL_INT = relevel(factor(POL_INT, levels = 
                                 c("Very interested", 
                                   "Somewhat interested",
                                   "Mostly uninterested",
                                   "Not at all interested"
                                 )), ref = "Very interested"))

#Political ideology kept in original format 

df8 <- data %>%
  select(LEFT_RIGHT) %>%   
  mutate(
    LEFT_RIGHT = relevel(factor(LEFT_RIGHT, levels = c(
      "Strongly left leaning", "Fairly left leaning", "Slightly left leaning", 
      "Neither left nor right leaning", "Slightly right leaning", 
      "Fairly right leaning", "Strongly right leaning", "I don't know")), 
      ref = "Strongly left leaning"
    )
  )


#Political ideology categorical with 8 categories (reduced to 4 for LR)

df2 <- data %>%
  data.frame() %>%
  select(LEFT_RIGHT)

#change left_right from 'don't know' to blank for easier conversion

df2$LEFT_RIGHT[df2$LEFT_RIGHT == "I don't know"] <- ""

#convert to numbers

df2 <- df2 %>%
  select(LEFT_RIGHT) %>%
  mutate_all(~c(99, -3, -2, -1, 0, 1, 2, 3)[match(.x, c("", "Strongly left leaning", "Fairly left leaning", "Slightly left leaning", "Neither left nor right leaning", "Slightly right leaning", "Fairly right leaning", "Strongly right leaning"
))])

#categorical in 4 categories for analysis

df2$LEFT_RIGHT[df2$LEFT_RIGHT == -3] <- 'Left leaning'
df2$LEFT_RIGHT[df2$LEFT_RIGHT == -2] <- 'Left leaning'
df2$LEFT_RIGHT[df2$LEFT_RIGHT == -1] <- 'Left leaning'
df2$LEFT_RIGHT[df2$LEFT_RIGHT == 0] <- 'Neither left nor right leaning'
df2$LEFT_RIGHT[df2$LEFT_RIGHT == 1] <- 'Right leaning'
df2$LEFT_RIGHT[df2$LEFT_RIGHT == 2] <- 'Right leaning'
df2$LEFT_RIGHT[df2$LEFT_RIGHT == 3] <- 'Right leaning'
df2$LEFT_RIGHT[df2$LEFT_RIGHT == 99] <- 'Not sure'

names(df2)[1] <- 'CAT_LEFT_RIGHT'

##need to also be as factor and reference group set

df2$CAT_LEFT_RIGHT <- as.factor(df2$CAT_LEFT_RIGHT)

df2 <- df2 %>%
  mutate(
    CAT_LEFT_RIGHT = relevel(factor(CAT_LEFT_RIGHT, levels = 
                                 c("Left leaning", "Right leaning", 
                                   "Neither left nor right leaning", "Not sure"
                                 )), ref = "Neither left nor right leaning"
    )
  )

```

```{r regulation trust}

#For this variable there is 1 missing piece of data, and 8 participants selected 'don't know' as their response. These 8 responses are changed to 99 in the data frame, and then converted to the neutral value of 4 - all analysis with this variable are checked with these 8 responses excluded.

df3 <- data %>%
  select(REG_TRUST_1)

df3 <- df3 %>%
  mutate(REG_TRUST_1 = c(1, 2, 3, 4, 5, 6, 7, 4, 4)[
    match(REG_TRUST_1, c("Strongly disagree", "Disagree", "Somewhat disagree",
                      "Neither agree nor disagree", "Somewhat agree", "Agree",
                      "Strongly agree", "", "Don't know"))
    ])

```


```{r demographics}

#creating dummy variables for gender and education

df4 <- data %>%
  data.frame() %>%
  select(GENDER, EDUCATION)

#gender

df4 <- df4 %>% 
  mutate(
    GENDER = relevel(factor(GENDER, levels = 
                                 c("Female", 
                                   "Male",
                                   "Non-binary / third gender",
                                   "Prefer not to say"
                                 )), ref = "Female"))

#education

df4 <- df4 %>%
 mutate(
    EDUCATION = relevel(factor(EDUCATION, levels = 
                              c("Postgraduate (e.g., M.Sc or Ph.D)",
                                "Undergraduate University (e.g., BA, B.Sc or B.Ed)",
                                "A level, or equivelent", "GCSE level, or equivelent",
                                "No formal qualifications",
                                "Other, please specify"
                              )), ref = "Postgraduate (e.g., M.Sc or Ph.D)"
    )
  )

#mean scoring age

df5 <- data %>%
  data.frame() %>%
  select(AGE)

df5 <- df5 %>%
  mutate(C_AGE = AGE)

df5$C_AGE <- df5$C_AGE - mean(df5$C_AGE)

```

```{r time spent on page}

#time spent on page

df6 <- data %>%
  select(Q54_Page.Submit, Q56_Page.Submit)

df6 <- pivot_longer(df6, cols = c(Q54_Page.Submit, Q56_Page.Submit), names_to = "response_t", values_to = "response_sec")

df6 <- df6 %>%
  mutate(across(response_sec, ~na_if(.x, "")))

df6 <- df6 %>%
  filter(!is.na(response_sec))

```

```{r combining the dataframe}

final <- cbind(final, df1, df2, df3, df4, df5, df6, df8)

final$response_sec <- as.numeric(final$response_sec)

# Convert response_sec from seconds to minutes
final$response_min <- final$response_sec / 60

```

```{r splitting sample to experimental condition}

df_recall <- final %>%
  subset(Condition == "transparent")

df_recall$Condition <- droplevels(df_recall$Condition)
df_recall$PAID_diff <- droplevels(df_recall$PAID_diff)
df_recall$TAR_diff <- droplevels(df_recall$TAR_diff)

```

### Logistic Regression Model

Aspects compared: agreement with campaign material, time spent on page, political interest, confidence in regulation, gender, age, education.

```{r logit model}

#run the original model

comparison_model <- glm(PAID_diff ~ pol_orientation + DATA_ACCEPT + AGREE + response_min + TRANSPARENT + SELF_TAR + POL_INT + LEFT_RIGHT + REG_TRUST_1 + GENDER + EDUCATION + C_AGE, data = df_recall, family = binomial(), control = glm.control(maxit = 50))

tab_model(comparison_model)

# Using stepwise regression to simplify the model
step_model <- step(comparison_model)

final_model <- glm(PAID_diff ~ AGREE + response_min + TRANSPARENT + LEFT_RIGHT + REG_TRUST_1, data = df_recall, family = binomial(), control = glm.control(maxit = 50))

# Check for multicollinearity using VIF
library(car)
vif(step_model)

tab_model(final_model)

summary(final_model)

# Extract p-values from the summary of the logistic regression model
p_vals <- summary(step_model)$coefficients[, "Pr(>|z|)"]

# Adjust p-values using the Benjamini-Hochberg method for FDR
adjusted_p_vals <- p.adjust(p_vals, method = "fdr")

# View the adjusted p-values
formatted_p_vals <- format(round(adjusted_p_vals, 5), scientific = FALSE)
formatted_p_vals

```

What I find from the various models is unclear - the presence of absense of the LEFT_RIGHT effect changes the significance level of the agreement effect. It is speculated the model may struggle to fit the agreement association due to the extreme results observed in the 'strongly right wing' group which agreed with the campaign material significantly less than all other groups. 

```{r}

# Create the scatterplot with jitter
ggplot(df_recall, aes(x = LEFT_RIGHT, y = AGREE)) +
  geom_jitter(width = 0.2, height = 0, alpha = 0.6, color = "blue") +  # Jitter to avoid overlap
  labs(title = "Scatterplot of AGREE by LEFT_RIGHT",
       x = "LEFT_RIGHT",
       y = "AGREE") +
  theme_minimal()  

# Create the scatterplot with jitter
ggplot(df_recall, aes(x = LEFT_RIGHT, y = PAID_diff)) +
  geom_jitter(width = 0.2, height = 0, alpha = 0.6, color = "blue") +  # Jitter to avoid overlap
  labs(title = "Scatterplot of sponsorship recall by LEFT_RIGHT",
       x = "LEFT_RIGHT",
       y = "AGREE") +
  theme_minimal()  

```

### Demographic differences in trust

```{r}

transparent <- lm(data=final, TRANSPARENT ~ pol_orientation + DATA_ACCEPT + SELF_TAR + POL_INT + LEFT_RIGHT + REG_TRUST_1 + GENDER + EDUCATION + C_AGE)

tab_model(transparent)

```


### Properly specifying the SEM model

```{r}

mediation <- final %>%
  select(Condition, AGREE, AD_TRUST_4, AD_TRUST_5, MANIPULATIVE, CREDIBLE, COMPTRUST_1, COMPTRUST_2, COMPDIS_1, COMPDIS_2, INTTRUST_1, INTTRUST_2, INTDIS_1, INTDIS_2, BENTRUST_1, BENTRUST_2, BENDIS_1, BENDIS_2)

# Specify the CFA model
measurement_model <- '
  # Latent construct for perceived transparency
  transparency =~ AD_TRUST_4 + AD_TRUST_5
  
  # Latent constructs for trust dimensions
  trust =~ COMPTRUST_1 + COMPTRUST_2 + INTTRUST_1 + INTTRUST_2 + BENTRUST_1 + BENTRUST_2
  distrust =~ COMPDIS_2 + INTDIS_1 + INTDIS_2 + BENDIS_1 + BENDIS_2
'

# Fit the CFA model
fit_measurement <- cfa(measurement_model, data = mediation)

# Inspect the covariance matrix of the latent variables
lavInspect(fit_measurement, "cov.lv")

# Summary with fit indices and standardized loadings
summary(fit_measurement, fit.measures = TRUE, standardized = TRUE)

# Specify the mediation model

mediation_model <- '
  # Measurement model (latent variables)
  transparency =~ AD_TRUST_4 + AD_TRUST_5
  trust =~ COMPTRUST_1 + COMPTRUST_2 + INTTRUST_1 + INTTRUST_2 + BENTRUST_1 + BENTRUST_2
  distrust =~ COMPDIS_2 + INTDIS_1 + INTDIS_2 + BENDIS_1 + BENDIS_2
  
  # Structural model (direct and indirect effects)
  transparency ~ a*Condition          
  trust ~ b*transparency + c*Condition  
  distrust ~ d*transparency + e*Condition 
  
  # Define the indirect effects
  indirect_trust := a * b
  indirect_distrust := a * d
  
  # Define the total effects
  total_trust := c + (a * b)
  total_distrust := e + (a * d)
'

mediation_model <- '
  # Measurement model (latent variables)
  transparency =~ AD_TRUST_4 + AD_TRUST_5
  trust =~ COMPTRUST_1 + COMPTRUST_2 + INTTRUST_1 + INTTRUST_2 + BENTRUST_1 + BENTRUST_2
  distrust =~ COMPDIS_2 + INTDIS_1 + INTDIS_2 + BENDIS_1 + BENDIS_2
  
  # Structural model (direct and indirect effects)
  transparency ~ a*Condition + f*AGREE   
  trust ~ b*transparency + c*Condition + g*AGREE  
  distrust ~ d*transparency + e*Condition + h*AGREE  
  
  # Define the indirect effects
  indirect_trust := a * b
  indirect_distrust := a * d
  
  # Define the total effects
  total_trust := c + (a * b)
  total_distrust := e + (a * d)
'

# Fit the mediation model
fit_mediation <- sem(mediation_model, data = mediation)

summary(fit_mediation, fit.measures = TRUE, standardized = TRUE)

# Extract parameter estimates (including p-values and standardized estimates)
estimates <- parameterEstimates(fit_mediation, standardized = TRUE)
# Filter to only include regression paths (latent-to-latent or observed-to-latent)
regression_paths <- estimates[estimates$op == "~", ]

# View the regression paths with their estimates and significance
print(regression_paths)

#plot

# Create the mediation plot with only latent variables and structural paths
lavaanPlot(model = fit_mediation, 
           coefs = TRUE,
           stand = TRUE,
           labels = TRUE,              
           graph_options = list(layout = "dot"), 
           node_options = list(shape = "ellipse", color = "lightblue"),
           edge_options = list(color = "black", arrowsize = 1))


```


